# -*- coding: utf-8 -*-
"""banana_leaf_diseases_model_image_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c8B8GCEWzZqGGL6HgOiI2pmKhVIEiMsW
"""

!pip install --upgrade Pillow
from PIL import Image
import os

input_root = '/content/Dataset'
output_root = '/content/Recorte y redimensionado'


os.makedirs(output_root, exist_ok=True)


def crop_and_resize(image, target_size=(224, 224)):
    width, height = image.size
    new_side = min(width, height)
    left = (width - new_side) // 2
    top = (height - new_side) // 2
    right = (width + new_side) // 2
    bottom = (height + new_side) // 2

    image_cropped = image.crop((left, top, right, bottom))

    image_resized = image_cropped.resize(target_size, Image.LANCZOS)

    return image_resized

for subdir, _, files in os.walk(input_root):
    for file in files:
        if file.endswith('.jpg') or file.endswith('.png'):
            input_path = os.path.join(subdir, file)

            relative_path = os.path.relpath(subdir, input_root)
            output_dir = os.path.join(output_root, relative_path)
            os.makedirs(output_dir, exist_ok=True)
            output_path = os.path.join(output_dir, file)

            img = Image.open(input_path)

            resized_img = crop_and_resize(img)

            resized_img.save(output_path)

print("Redimensionado completado.")

import os
import shutil
import random

dataset_root = '/content/Recorte y redimensionado'
balanced_dataset_root = '/content/Balanceado'

os.makedirs(balanced_dataset_root, exist_ok=True)


classes = [d for d in os.listdir(dataset_root) if os.path.isdir(os.path.join(dataset_root, d))]

class_counts = {}
for cls in classes:
    class_path = os.path.join(dataset_root, cls)
    class_counts[cls] = len(os.listdir(class_path))

min_count = min(class_counts.values())

for cls in classes:
    class_path = os.path.join(dataset_root, cls)
    balanced_class_path = os.path.join(balanced_dataset_root, cls)

    os.makedirs(balanced_class_path, exist_ok=True)

    images = os.listdir(class_path)

    if len(images) > min_count:
        images = random.sample(images, min_count)

    for image in images:
        src_path = os.path.join(class_path, image)
        dest_path = os.path.join(balanced_class_path, image)
        shutil.copy(src_path, dest_path)

print("Dataset balanceado y guardado en:", balanced_dataset_root)

import numpy as np
import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array

base_dir = '/content/drive/My Drive/Dataset_Balanceado/'
classes = ['BLACK_SIGATOKA', 'HEALTHY', 'MAL_DE_PANAMA']

datagen = ImageDataGenerator(
    rotation_range=40,
    vertical_flip=True,
    zoom_range=0.2
)

output_dir = '/content/drive/My Drive/Dataset_Aumentado/'

for cls in classes:
    cls_dir = os.path.join(base_dir, cls)
    output_cls_dir = os.path.join(output_dir, cls)

    images = [f for f in os.listdir(cls_dir) if f.endswith('.jpg') or f.endswith('.png')]

    generated_images = 0

    for img_name in images:
        img_path = os.path.join(cls_dir, img_name)
        img = load_img(img_path)
        x = img_to_array(img)
        x = np.expand_dims(x, axis=0)

        for batch in datagen.flow(x, batch_size=1):
            generated_img_name = f"{os.path.splitext(img_name)[0]}_aug_{generated_images}.jpg"
            generated_img_path = os.path.join(output_cls_dir, generated_img_name)
            plt.imsave(generated_img_path, batch[0].astype(np.uint8))

            generated_images += 1

            if generated_images >= 701:
                break
        if generated_images >= 701:
            break

print("Aumento de datos completado.")

import os
import shutil
import random

def split_dataset(dataset_dir, output_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    classes = os.listdir(dataset_dir)

    for class_name in classes:
        class_dir = os.path.join(dataset_dir, class_name)
        images = os.listdir(class_dir)
        random.shuffle(images)

        train_split = int(train_ratio * len(images))
        val_split = int(val_ratio * len(images)) + train_split

        train_images = images[:train_split]
        val_images = images[train_split:val_split]
        test_images = images[val_split:]

        for folder_name, image_list in zip(['train', 'val', 'test'], [train_images, val_images, test_images]):
            folder_path = os.path.join(output_dir, folder_name, class_name)
            os.makedirs(folder_path, exist_ok=True)

            for image_name in image_list:
                src = os.path.join(class_dir, image_name)
                dst = os.path.join(folder_path, image_name)
                shutil.copy2(src, dst)

dataset_dir = '/content/drive/My Drive/Dataset_Aumentado/'
output_dir = '/content/drive/My Drive/Dataset_Final/'
split_dataset(dataset_dir, output_dir)